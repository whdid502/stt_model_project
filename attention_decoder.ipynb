{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PFtjdwSnG8Ik"
      ],
      "authorship_tag": "ABX9TyNlrzKmhqqPf09AJffdeMxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/decoder/attention_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7cVIQlwKPi"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRK4o0qwPbf"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDFmoxfEwNjV"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYrXsg3wYLz"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool):\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypewFuYwMEq"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        self.device = device\n",
        "\n",
        "        if self.extractor == 'vgg':\n",
        "            input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "    conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "    if self.training:\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "    return output, hidden\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        self.wq = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = torch.nn.Linear(d_model, d_model, bias=True) # ??\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :        \n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention) # TODO : check\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE"
      },
      "source": [
        "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "# out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "# display(out.shape, attn.shape)\n",
        "# display(y)\n",
        "# out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderStep(torch.nn.Module) :\n",
        "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
        "        super(DecoderStep, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = torch.nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        self.layernorm1 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var).to(self.device)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if self.training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = torch.nn.functional.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(torch.nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        # self.num_layers = num_layers\n",
        "\n",
        "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs) :\n",
        "        assert enc_outputs is not None or inputs is not None\n",
        "\n",
        "        hidden = None\n",
        "        result, decode_dict = list(), dict()\n",
        "\n",
        "        if not self.training :\n",
        "            decode_dict[Speller.KEY_ATTENTION_SCORE] = list()\n",
        "            decode_dict[Speller.KEY_SEQUENCE_SYMBOL] = list()\n",
        "\n",
        "        max_lengths = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # TODO : delete\n",
        "        print(\"🎶 input_var size : \", input_var.size())\n",
        "\n",
        "        for di in range(max_lengths) :\n",
        "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "            if not self.training :\n",
        "                decode_dict[Speller.KEY_ATTENTION_SCORE].append(attn)\n",
        "                decode_dict[Speller.KEY_SEQUENCE_SYMBOL].append(input_var)\n",
        "                eos_batches = input_var.data.eq(2) # eq(eos_id)\n",
        "\n",
        "                if eos_batches.dim() > 0 :\n",
        "                    eos_batches = eos_batches.cpu().view(-1).numpy()\n",
        "                    update_idx = ((lengths > di) & eos_batches) != 0\n",
        "                    lengths[update_idx] = len(decode_dict[Speller.KEY_SEQUENCE_SYMBOL])\n",
        "\n",
        "        del decode_dict # TODO : check that this is necessary\n",
        "\n",
        "        return result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VWgRi5H-Vp"
      },
      "source": [
        "class LAS(torch.nn.Module) :\n",
        "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
        "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
        "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
        "\n",
        "        # TODO : check\n",
        "        # flatten parameter\n",
        "        self.encoder.rnn.flatten_parameters()\n",
        "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "    def forward(self, inputs, input_lengths, targets=None):\n",
        "        output, hidden = self.encoder(inputs, input_lengths)\n",
        "        print(\"😎 encoding done -> output size : \", output.size())\n",
        "\n",
        "        result = self.decoder(targets, output)\n",
        "        print(\"🧐 decoder done\")\n",
        "\n",
        "        return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbf3XKq4D4bO"
      },
      "source": [
        "## data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGVXqcTD5y3"
      },
      "source": [
        "# TODO\n",
        "# 0. csv 파일 내에 있는 data path를 통해 audio->feature vector & label 을 tuple로 묶어서 list에 넣어두기\n",
        "# 1. max length로 padding"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtjdwSnG8Ik"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XlgaQuTJarq"
      },
      "source": [
        "# def train_step(model, epoch, inputs, targets, device='cuda') :\n",
        "#     cer = 1.0\n",
        "#     epoch_loss_total = 0.\n",
        "#     total_num = 0\n",
        "#     # timestep = 0\n",
        "\n",
        "#     train_start_time = time.time()\n",
        "\n",
        "#     model.train() # model을 train mode로 변경\n",
        "\n",
        "#     inputs = inputs.to(device)\n",
        "#     targets = targets.to(device)\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     if isinstance(model, nn.DataParallel):\n",
        "#         model.module.flatten_parameters()\n",
        "#     else:\n",
        "#         model.flatten_parameters()\n",
        "\n",
        "\n",
        "#     result = model(inputs, input_lengths, targets)\n",
        "#     result = torch.stack(result, dim=1).to(device)\n",
        "#     targets = targets[:, 1:] # TODO : check that this is necessary -> 맨 앞에 하나를 빼고 target을 넣음????\n",
        "\n",
        "#     top_1 = result.max(-1)[1]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvUt90vFLWy5"
      },
      "source": [
        "# output = net(input)\n",
        "# target = torch.randn(10)  # a dummy target, for example\n",
        "# target = target.view(1, -1)  # make it the same shape as output\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# loss = criterion(output, target)\n",
        "# print(loss)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5jdW4rFwQB"
      },
      "source": [
        "# def train(model, batch_size, num_epochs) :\n",
        "#     print(\"[INFO] train start\")\n",
        "\n",
        "#     for epoch in range(num_epochs) :\n",
        "#         # train\n",
        "#         epoch_loss, epoch_cer = train_step(model, epoch)\n",
        "\n",
        "#         # checkpoint 저장\n",
        "#         if (epoch+1) % 5 == 0 :\n",
        "#             pass\n",
        "\n",
        "#         # print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "#         #                                         train_loss.result(), \n",
        "#         #                                         train_accuracy.result()))\n",
        "\n",
        "#         # print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.module.parameter(), lr=learning_rate, weight_decay=weight_decay)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIsWyc8HvXO1"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMNSrRyD73h"
      },
      "source": [
        "# hyper-parameter\n",
        "INPUT_SIZE = 80\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT_P = 0.15\n",
        "MAX_LEN = 150\n",
        "NUM_HEADS = 4\n",
        "ENC_NUM_LAYERS = 3\n",
        "DEC_NUM_LAYERS = 2\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = 50 # TODO : dataset으로 label의 개수 넣어주기\n",
        "LEARNING_RATE = 1e-06\n",
        "WEIGHT_DECAY = 1e-05\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79",
        "outputId": "4adf639f-b077-4d76-a0d1-4043c8494b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "tmp_input = torch.rand((BATCH_SIZE,951,INPUT_SIZE), dtype=torch.float64).uniform_(0,200)\n",
        "tmp_input_length = torch.randint(0, INPUT_SIZE, size=(BATCH_SIZE,))\n",
        "tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
        "\n",
        "tmp_input = tmp_input.float()\n",
        "tmp_target = tmp_target.long()\n",
        "\n",
        "model = torch.nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=INPUT_SIZE, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=12, \n",
        "                                  num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "\n",
        "print(\"model 초기화 성공\")\n",
        "\n",
        "tmp_input.to('cuda')\n",
        "tmp_target.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "fn_out = model(tmp_input, tmp_input_length, tmp_target)\n",
        "\n",
        "len(fn_out)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model 초기화 성공\n",
            "😎 encoding done -> output size :  torch.Size([8, 237, 512])\n",
            "🎶 input_var size :  torch.Size([8, 1])\n",
            "🧐 decoder done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuFO-Q-zqRz"
      },
      "source": [
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}