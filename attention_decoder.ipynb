{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(torch.nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = torch.nn.Linear(d_model, d_model, bias=True) # ??\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wq(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wq(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention) # ??\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE",
        "outputId": "b69847f8-feb7-4cf7-9a25-3e2c1832c015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "display(out.shape, attn.shape)\n",
        "display(y)\n",
        "out"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([1, 60, 512])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([8, 60, 60])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[[0.4955, 0.2698, 0.0228,  ..., 0.3468, 0.9099, 0.3282],\n",
              "         [0.3973, 0.7943, 0.4576,  ..., 0.6460, 0.6003, 0.4776],\n",
              "         [0.8922, 0.0058, 0.7936,  ..., 0.4749, 0.1825, 0.7920],\n",
              "         ...,\n",
              "         [0.4431, 0.0522, 0.9016,  ..., 0.9424, 0.2426, 0.5663],\n",
              "         [0.5014, 0.5712, 0.6320,  ..., 0.9312, 0.5130, 0.4896],\n",
              "         [0.4478, 0.1749, 0.9620,  ..., 0.9004, 0.7717, 0.8591]]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0054, -0.1599, -0.0259,  ..., -0.1342,  0.3016,  0.1085],\n",
              "         [-0.0043, -0.1593, -0.0252,  ..., -0.1349,  0.3035,  0.1090],\n",
              "         [-0.0065, -0.1619, -0.0244,  ..., -0.1344,  0.3040,  0.1080],\n",
              "         ...,\n",
              "         [-0.0054, -0.1629, -0.0252,  ..., -0.1342,  0.3032,  0.1081],\n",
              "         [-0.0041, -0.1610, -0.0249,  ..., -0.1353,  0.3033,  0.1094],\n",
              "         [-0.0052, -0.1614, -0.0252,  ..., -0.1342,  0.3028,  0.1093]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderLayer(torch.nn.Module) :\n",
        "    def __init__(self, num_classes, d_model=1024, num_heads=4, dropout_p=0.3):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = torch.nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=1, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        self.layernorm1 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output, training) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = torch.nn.functional.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lenghts, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(torch.nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, num_layers=2, dropout_p=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # self.dec_layers = [DecoderLayer(d_model, num_heads, num_classes, dropout_p) for _ in range(num_layers)]\n",
        "        self.dec_layer = DecoderLayer(d_model, num_heads, num_classes, dropout_p)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs, training) :\n",
        "        assert enc_outputs is not None or inputs is not None\n",
        "        hidden = None\n",
        "        result = list()\n",
        "\n",
        "        max_lengths = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "\n",
        "        for di in range(max_lengths):\n",
        "            step_output, hidden = self.dec_layer(input_var, hidden, enc_outputs, training)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "        return result"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}