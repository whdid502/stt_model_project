{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1Rje1P5tIJvYWcKX5JQ0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/decoder/attention_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, optim\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7cVIQlwKPi"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRK4o0qwPbf"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDFmoxfEwNjV"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYrXsg3wYLz"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool):\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypewFuYwMEq"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        self.device = device\n",
        "\n",
        "        if self.extractor == 'vgg':\n",
        "            input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "    conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "    if self.training:\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "    return output, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        self.wq = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, d_model, bias=True) # ??\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :        \n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention) # TODO : check\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE"
      },
      "source": [
        "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "# out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "# display(out.shape, attn.shape)\n",
        "# display(y)\n",
        "# out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderStep(nn.Module) :\n",
        "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
        "        super(DecoderStep, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var).to(self.device)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if self.training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        # self.num_layers = num_layers\n",
        "\n",
        "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs) :\n",
        "        assert enc_outputs is not None or inputs is not None\n",
        "\n",
        "        hidden = None\n",
        "        result = list()\n",
        "\n",
        "        max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "        batch_size = enc_outputs.size(0)\n",
        "        lengths = np.array([max_length] * batch_size)\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "        \n",
        "        # TODO : delete\n",
        "        print(\"üé∂ input_var size : \", input_var.size())\n",
        "\n",
        "        for di in range(max_length) :\n",
        "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "            # TODO : ??\n",
        "            if not self.training :\n",
        "                eos_batches = input_var.data.eq(2) # eq(eos_id)\n",
        "\n",
        "                if eos_batches.dim() > 0 :\n",
        "                    eos_batches = eos_batches.cpu().view(-1).numpy()\n",
        "                    update_idx = ((lengths > di) & eos_batches) != 0\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VWgRi5H-Vp"
      },
      "source": [
        "class LAS(nn.Module) :\n",
        "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
        "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
        "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, input_lengths, targets=None):\n",
        "        output, hidden = self.encoder(inputs, input_lengths)\n",
        "        # print(\"üòé encoding done -> output size : \", output.size())\n",
        "\n",
        "        result = self.decoder(targets, output)\n",
        "        # print(\"üßê decoder done\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def flatten_parameters(self) :\n",
        "        self.encoder.rnn.flatten_parameters()\n",
        "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbf3XKq4D4bO"
      },
      "source": [
        "## data Ï§ÄÎπÑ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGVXqcTD5y3"
      },
      "source": [
        "# TODO\n",
        "# 0. csv ÌååÏùº ÎÇ¥Ïóê ÏûàÎäî data pathÎ•º ÌÜµÌï¥ audio->feature vector & label & len(feature_vectors) & len(labels) Í∞ÅÍ∞ÅÏùò listÎ•º ÏùÑ ÌïòÎÇòÏùò tupleÎ°ú Î¨∂Í∏∞\n",
        "# 1. max lengthÎ°ú padding\n",
        "# 2. id2char dictionary ÌïÑÏöî"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkoAlbfPNL8"
      },
      "source": [
        "# !pip install python-Levenshtein\n",
        "import Levenshtein as Lev\n",
        "\n",
        "total_dist = 0.0\n",
        "total_length = 0.0\n",
        "EOS_ID = 0\n",
        "id2char = {} # TODO : delete\n",
        "\n",
        "def label_to_string(labels) :    \n",
        "    sentences = str()\n",
        "    for label in labels :\n",
        "        if label.item() == EOS_ID :\n",
        "            break\n",
        "        sentence += id2char[label.item()]\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "def charErrorRate(targets, hypothesises) :\n",
        "    for target, hypothesis in zip(targets, hypothesises) :\n",
        "        s1 = label_to_string(target)\n",
        "        s2 = label_to_string(hypothesis)\n",
        "\n",
        "        # space Ï†úÍ±∞\n",
        "        s1 = s1.replace(' ', '')\n",
        "        s2 = s2.replace(' ', '')\n",
        "\n",
        "        # TODO : check -> sentenceÏóê '_'Í∞Ä ÏûàÎäîÏßÄ ÏóÜÎäîÏßÄ ÌôïÏù∏, ÏûàÎã§Î©¥ '_'ÎèÑ ÏßÄÏö∞Í∏∞\n",
        "\n",
        "        dist = Lev.distance(s2, s1)\n",
        "        length = len(s1)\n",
        "\n",
        "        total_dist += dist\n",
        "        total_length += length\n",
        "\n",
        "    return total_dist / total_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtjdwSnG8Ik"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XlgaQuTJarq"
      },
      "source": [
        "def train_step(model, epoch, train_dataset, loss_func, optimizer, device='cuda') :\n",
        "    inputs, input_lengths, targets, target_lengths = train_dataset\n",
        "    \n",
        "    model.train() # modelÏùÑ train modeÎ°ú Î≥ÄÍ≤Ω\n",
        "    \n",
        "    train_start_time = time.time()\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.flatten_parameters()\n",
        "    else :\n",
        "        model.flatten_parameters()\n",
        "\n",
        "    result = model(inputs, input_lengths, targets)\n",
        "    result = torch.stack(result, dim=1).to(device) # listÎ•º dim=1 Î∞©Ìñ•ÏúºÎ°ú concatenate => return Tensor\n",
        "    hypothesises = result.max(-1)[1]\n",
        "\n",
        "    # loss Í≥ÑÏÇ∞\n",
        "    targets = targets[:, 1:] # 0Î≤àÏß∏ columnÏùÑ Î∫å (Î™®Îëê 1ÏûÑ.)\n",
        "    loss = loss_func(result.contiguous().view(-1, result.size(-1)), targets.contiguous().view(-1))\n",
        "    step_loss = loss.item() # loss.item()ÏùÄ lossÏùò Ïä§ÏπºÎùº Í∞í.\n",
        "\n",
        "    # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
        "    cer = charErrorRate(targets, hypothesises)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
        "\n",
        "    # torch.cuda.empty_cache() # TODO : check that this is necessary\n",
        "\n",
        "    return step_loss, cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5jdW4rFwQB"
      },
      "source": [
        "def train(model, batch_size, num_epochs, lr, weight_decay) :\n",
        "    print(\"[INFO] train start\")\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "    optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(num_epochs) :\n",
        "        # TODO\n",
        "        # 1. batch sizeÎßåÌÅº data loadÌïòÍ∏∞ \n",
        "        # -> csv ÌååÏùº ÎÇ¥Ïóê ÏûàÎäî data pathÎ•º ÌÜµÌï¥ audio->feature vector & label & len(feature_vectors) & len(labels) Í∞ÅÍ∞ÅÏùò listÎ•º ÏùÑ ÌïòÎÇòÏùò tupleÎ°ú Î¨∂Í∏∞\n",
        "        tmp_input = torch.rand((BATCH_SIZE,951,N_MELS), dtype=torch.float64).uniform_(0,200)\n",
        "        tmp_input_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "        tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
        "        tmp_target_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "\n",
        "        tmp_input = tmp_input.float()\n",
        "        tmp_target = tmp_target.long()\n",
        "\n",
        "        train_dataset = (tmp_input, tmp_input_length, tmp_target, tmp_target_length)\n",
        "\n",
        "        # train\n",
        "        epoch_loss, epoch_cer = train_step(model, epoch, train_dataset, loss_func, optimizer)\n",
        "\n",
        "        # checkpoint Ï†ÄÏû•\n",
        "        if (epoch+1) % 5 == 0 :\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIsWyc8HvXO1"
      },
      "source": [
        "## Ïã§Ìñâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMNSrRyD73h"
      },
      "source": [
        "# hyper-parameter\n",
        "N_MELS = 80\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT_P = 0.15\n",
        "MAX_LEN = 150\n",
        "NUM_HEADS = 4\n",
        "ENC_NUM_LAYERS = 3\n",
        "DEC_NUM_LAYERS = 2\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = 50 # TODO : datasetÏúºÎ°ú labelÏùò Í∞úÏàò ÎÑ£Ïñ¥Ï£ºÍ∏∞\n",
        "LEARNING_RATE = 1e-06\n",
        "WEIGHT_DECAY = 1e-05\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79",
        "outputId": "6cca36f0-e916-4d20-ccaf-535392151cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=12, \n",
        "                                  num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "\n",
        "print(\"model Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ\")\n",
        "\n",
        "train(model, BATCH_SIZE, 1, LEARNING_RATE, WEIGHT_DECAY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ\n",
            "[INFO] train start\n",
            "üé∂ input_var size :  torch.Size([8, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1838.8216552734375"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuFO-Q-zqRz"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}