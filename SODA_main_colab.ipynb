{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SODA_main_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qt7cVIQlwKPi",
        "GM11tHrMpzIR",
        "iwv1JKKfRuRv"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1ApGOIpngPOem8qf2KiA5sVLIVa8gtoYO",
      "authorship_tag": "ABX9TyN4oFXmb6SqqjOuJ8PMSBfL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/master/SODA_main_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, optim\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7cVIQlwKPi"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRK4o0qwPbf"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDFmoxfEwNjV"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYrXsg3wYLz"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool) :\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypewFuYwMEq"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        self.device = device\n",
        "\n",
        "        if self.extractor == 'vgg':\n",
        "            # input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            input_size = 1024\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "    conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "    if self.training:\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "    return output, hidden"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfzxPWgIZEQj"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        self.wq = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :        \n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE"
      },
      "source": [
        "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "# out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "# display(out.shape, attn.shape)\n",
        "# display(y)\n",
        "# out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderStep(nn.Module) :\n",
        "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
        "        super(DecoderStep, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var).to(self.device)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if self.training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "        # self.num_layers = num_layers\n",
        "\n",
        "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs) :\n",
        "        assert enc_outputs is not None\n",
        "        \n",
        "        hidden = None\n",
        "        result = list()\n",
        "\n",
        "        batch_size = enc_outputs.size(0)\n",
        "\n",
        "        # validate\n",
        "        if inputs is None :\n",
        "            inputs = torch.LongTensor([1] * batch_size).view(batch_size, 1).to(self.device) # [sos_id] * batch_size\n",
        "            max_length = self.max_length\n",
        "        else :\n",
        "            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "        for di in range(max_length) :\n",
        "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "        return result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM11tHrMpzIR"
      },
      "source": [
        "## LAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VWgRi5H-Vp"
      },
      "source": [
        "class LAS(nn.Module) :\n",
        "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
        "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
        "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, input_lengths, targets=None):\n",
        "        output, hidden = self.encoder(inputs, input_lengths)\n",
        "\n",
        "        result = self.decoder(targets, output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def flatten_parameters(self) :\n",
        "        self.encoder.rnn.flatten_parameters()\n",
        "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbf3XKq4D4bO"
      },
      "source": [
        "## data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYHyk6D1LN9"
      },
      "source": [
        "import csv\n",
        "\n",
        "# (train & valid) dataset path가 들어있는 csv 파일 읽어오기\n",
        "# parameter로 train path csv 경로 혹은 valid path csv 경로가 들어옴\n",
        "# => return data_path_list\n",
        "def load_path_list(train_path, valid_path) :\n",
        "    train_path_list, valid_path_list = [], []\n",
        "\n",
        "    print(f\"[INFO] load train path list from {train_path}\")\n",
        "    with open(train_path, 'r') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            train_path_list.append((line[0], line[1]))\n",
        "\n",
        "    print(f\"[INFO] load train path list from {valid_path}\")\n",
        "    with open(valid_path, 'r') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            valid_path_list.append((line[0], line[1]))\n",
        "\n",
        "    return train_path_list, valid_path_list\n",
        "\n",
        "# id, char 가 적혀있는 csv 파일 읽어오기\n",
        "# => return id2char (-> type : dictionary)\n",
        "def load_id2char(path) :\n",
        "    print(f\"[INFO] load id2char from {path}\")\n",
        "    \n",
        "    id2char = {}\n",
        "\n",
        "    with open(path, 'r', encoding='ms949') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            id2char[line[0]] = line[1]\n",
        "    \n",
        "    return id2char"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ygnl0Uk8_p1"
      },
      "source": [
        "#### feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmEcNMPA9Crt",
        "outputId": "06d529ac-86e8-4b19-870b-dedd3aebcd5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.13.2\n",
        "!pip3 install SpecAugment"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 34kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.33.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.5)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n",
            "Collecting SpecAugment\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/32/c569482868868631df63fdd406854bd82be05362a7d8cb45b1970ea51374/SpecAugment-1.2.5-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from SpecAugment) (3.2.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (from SpecAugment) (0.6.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->SpecAugment) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->SpecAugment) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->SpecAugment) (1.18.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->SpecAugment) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->SpecAugment) (2.8.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (0.17.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (0.22.2.post1)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (0.48.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->SpecAugment) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa->SpecAugment) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa->SpecAugment) (50.3.2)\n",
            "Installing collected packages: SpecAugment\n",
            "Successfully installed SpecAugment-1.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DBbM9v3pHw3",
        "outputId": "2a0952ab-e7e4-43eb-cbc0-ce2e1c1e671d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')\n",
        "from specAugment import spec_augment_tensorflow"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDArguH69iEn"
      },
      "source": [
        "import librosa\n",
        "from scipy.fftpack import dct\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSd3pezNo63j"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')\n",
        "from specAugment import spec_augment_tensorflow"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYULYIur8rZ_"
      },
      "source": [
        "# audio -> feature vector\n",
        "# => return feature vector\n",
        "def audio_to_featureVector(audio_path, n_mels, noise_injection=False) :\n",
        "    signal = np.memmap(os.path.join(os.getcwd(), 'original', audio_path), dtype='h', mode='r').astype('float32') # load audio\n",
        "    data = signal / 32767   # normalize audio\n",
        "\n",
        "    # noise injection\n",
        "    if noise_injection :\n",
        "        wn = np.random.randn(len(data))\n",
        "        data_wn = data + 0.005*wn\n",
        "    else:\n",
        "        data_wn = data\n",
        "\n",
        "    sr = 16000\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=data_wn, sr=sr, n_mels=256, hop_length=128, fmax=8000) # data to melspectrogram\n",
        "\n",
        "    warped_masked_spectrogram = spec_augment_tensorflow.spec_augment(mel_spectrogram=mel_spectrogram, time_warping_para=30) # melspectrogram spec augmentation\n",
        "\n",
        "    mfcc = dct(librosa.core.power_to_db(warped_masked_spectrogram), type=2, axis=1, norm='ortho')[:n_mels] # mel spectrogram to mfcc\n",
        "\n",
        "    return mfcc\n",
        "\n",
        "# 0. batch size만큼의 data path list 받아서 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "# 1. batch내의 max length로 padding\n",
        "# => return inputs, input_lengths, targets, target_lengths\n",
        "def load_data(batch_data_path_list, n_mels):\n",
        "    mfcc_list, input_lengths, target_list, target_lengths = [], [], [], []\n",
        "\n",
        "    max_mfcc_shape = 0\n",
        "    max_target_shape = 0\n",
        "    for i in batch_data_path_list :\n",
        "        # audio to feature vector\n",
        "        mfcc = audio_to_featureVector(i[0], n_mels, False) \n",
        "        mfcc_list.append(mfcc)\n",
        "        input_lengths.append(mfcc.shape[1])\n",
        "        # for padding\n",
        "        if max_mfcc_shape < mfcc.shape[1] :\n",
        "            max_mfcc_shape = mfcc.shape[1]\n",
        "\n",
        "        # target list\n",
        "        with open(os.path.join(os.getcwd(), 'original', i[1]), 'r') as f :\n",
        "            label = f.readline()\n",
        "            label = list(map(int, label.split()))\n",
        "            target_list.append(label)\n",
        "            target_lengths.append(len(label))\n",
        "            # for padding\n",
        "            if max_target_shape < len(label) :\n",
        "                max_target_shape = len(label)\n",
        "\n",
        "    # train padding\n",
        "    inputs = []\n",
        "    for mfcc in mfcc_list :\n",
        "        padding_shape = np.zeros((n_mels, max_mfcc_shape))\n",
        "        padding_shape[:mfcc.shape[0],:mfcc.shape[1]] = mfcc\n",
        "        inputs.append(padding_shape)\n",
        "\n",
        "    # target padding\n",
        "    padding_targets = []\n",
        "    for target in target_list :\n",
        "        target_padding_shape = np.zeros(max_target_shape+2)\n",
        "        target.insert(0,1) # 맨 앞에 sos_id 추가\n",
        "        target.append(2) # 맨 뒤에 eos_id 추가\n",
        "        target_padding_shape[:len(target)] = target\n",
        "        padding_targets.append(target_padding_shape)\n",
        "\n",
        "    inputs = torch.FloatTensor(inputs).permute(0,2,1)\n",
        "    input_lengths = torch.IntTensor(input_lengths)\n",
        "    padding_targets = torch.LongTensor(padding_targets)\n",
        "\n",
        "    return inputs, input_lengths, padding_targets, target_lengths"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwv1JKKfRuRv"
      },
      "source": [
        "### 정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8bi0wDfB4yM",
        "outputId": "1a6564ae-78dd-4778-ea1c-7b849409e832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s \"/content/drive/My Drive/\" \"/myDrive\"\n",
        "!ls \"/myDrive\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'회의 보고서'\t\t      colab_pro_영수증_양문영.pdf\n",
            "'광주 인공지능 사관학교'     '도서 구매 증명서 양식_양문영.docx'\n",
            "'날씨의 요정'\t\t     '지원금 사용현황.gsheet'\n",
            "'20200820 발표 대본.gdoc'     양문영_신분증.jpeg\n",
            "'20200820 발표.gslides'       양문영_통장사본.jpeg\n",
            " 20201020_코랩_매출전표.jpg   KakaoTalk_20201022_143217683.jpg\n",
            " 20201022_회의록_24번.docx    양문영_주민등록초본.pdf\n",
            " 20201022_회의비_24번.jpg     SODA\n",
            "'Colab Notebooks'\t      yolov4\n",
            " colab_pro_결제.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifPs1l3pUiR",
        "outputId": "b0e18efc-311c-4ce5-f9c0-45b13daec446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd \"/myDrive/SODA/data\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SODA/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkoAlbfPNL8",
        "outputId": "49205376-d603-493a-b4e9-341acda08359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install python-Levenshtein\n",
        "import Levenshtein as Lev\n",
        "\n",
        "total_dist = 0.0\n",
        "total_length = 0.0\n",
        "EOS_ID = 2\n",
        "id2char = load_id2char(\"aihub_labels.csv\")\n",
        "\n",
        "def label_to_string(labels) : \n",
        "    if len(labels.shape) == 1:\n",
        "        sentence = str()\n",
        "        for label in labels:\n",
        "            if label.item() == EOS_ID :\n",
        "                break\n",
        "            if id2char[str(label.item())] == '^' :\n",
        "                sentence += \"(웃음)\"\n",
        "            elif str(label.item()) == '3' :\n",
        "                sentence += \" \"\n",
        "            else :\n",
        "                sentence += id2char[str(label.item())]\n",
        "\n",
        "        return sentence\n",
        "\n",
        "    sentences = list()\n",
        "    for batch in labels:\n",
        "        sentence = str()\n",
        "        for label in batch:\n",
        "            if label.item() == EOS_ID :\n",
        "                break\n",
        "            if id2char[str(label.item())] == '^' :\n",
        "                sentence += \"(웃음)\"\n",
        "            elif str(label.item()) == '3' :\n",
        "                sentence += \" \"\n",
        "            else :\n",
        "                sentence += id2char[str(label.item())]\n",
        "        sentences.append(sentence)\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def charErrorRate(targets, hypothesises) :\n",
        "    total_dist = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for target, hypothesis in zip(targets, hypothesises) :\n",
        "        s1 = label_to_string(target)\n",
        "        s2 = label_to_string(hypothesis)\n",
        "\n",
        "        # space 제거\n",
        "        s1 = s1.replace(' ', '')\n",
        "        s2 = s2.replace(' ', '')\n",
        "\n",
        "        # '_' 제거\n",
        "        s1 = s1.replace('_', '')\n",
        "        s2 = s2.replace('_', '')\n",
        "\n",
        "        dist = Lev.distance(s2, s1)\n",
        "        length = len(s1)\n",
        "\n",
        "        total_dist += dist\n",
        "        total_length += length\n",
        "\n",
        "    return 100 * (1 - total_dist/total_length)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144803 sha256=b6b5c2d08372c21328c1ef2448c72bfebec00584f29cc6309c15949cadc32ac0\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "[INFO] load id2char from aihub_labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsApHJTBU0l"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2pDfJGBWl6"
      },
      "source": [
        "CHECKPOINT_SAVE_PATH = '/content/drive/My Drive/SODA/backup'\n",
        "had_header = False\n",
        "\n",
        "def model_save(model, optimizer, loss_cer, train_index, valid_index) :\n",
        "    global had_header\n",
        "    date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime(time.time()))\n",
        "    \n",
        "    # if isinstance(model, nn.DataParallel):\n",
        "    #     trainer_states = {\n",
        "    #         'train_index' : train_index,\n",
        "    #         'valid_index' : valid_index,\n",
        "    #         'epoch' : loss_cer['epoch'],\n",
        "    #         'optimizer' : optimizer.module.state_dict(),\n",
        "    #         'model' : model.module.state_dict()\n",
        "    #     }\n",
        "    # else :\n",
        "    #     trainer_states = {\n",
        "    #         'train_index' : train_index,\n",
        "    #         'valid_index' : valid_index,\n",
        "    #         'epoch' : loss_cer['epoch'],\n",
        "    #         'optimizer' : optimizer.state_dict(),\n",
        "    #         'model' : model.state_dict()\n",
        "    #     }\n",
        "\n",
        "    trainer_states = {\n",
        "        'train_index' : train_index,\n",
        "        'valid_index' : valid_index,\n",
        "        'epoch' : loss_cer['epoch'],\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "        'model' : model.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(trainer_states, os.path.join(CHECKPOINT_SAVE_PATH, \"model_\"+date_time+\".pt\")) # model save\n",
        "    torch.save(model, os.path.join(CHECKPOINT_SAVE_PATH, \"total_model_\"+date_time+\".pt\")) # model save\n",
        "    print(\"model name : total_model_\"+date_time+\".pt\")\n",
        "\n",
        "    # loss & cer save\n",
        "    loss_cer['date_time'] = date_time\n",
        "    with open(os.path.join(CHECKPOINT_SAVE_PATH, 'loss_cer.csv'), 'a', encoding='ms949') as f :\n",
        "        w = csv.DictWriter(f, loss_cer.keys())\n",
        "\n",
        "        if not had_header :\n",
        "            w.writeheader()\n",
        "            had_header = True\n",
        "\n",
        "        w.writerow(loss_cer)\n",
        "\n",
        "def model_load(model_name) :\n",
        "    checkpoint = torch.load(os.path.join(CHECKPOINT_SAVE_PATH, model_name))\n",
        "    return checkpoint\n",
        "\n",
        "# TODO : checkpoint가 저장되는 형식보고 작성하기\n",
        "def model_load_latest(checkpoint_dir) :\n",
        "    pass"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtjdwSnG8Ik"
      },
      "source": [
        "## Train & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XlgaQuTJarq"
      },
      "source": [
        "def train_step(model, epoch, train_dataset, loss_func, optimizer, device='cuda') :\n",
        "    model.train() # model을 train mode로 변경\n",
        "    \n",
        "    inputs, input_lengths, targets, target_lengths = train_dataset\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.flatten_parameters()\n",
        "    else :\n",
        "        model.flatten_parameters()\n",
        "\n",
        "    result = model(inputs, input_lengths, targets)\n",
        "    result = torch.stack(result, dim=1).to(device) # list를 dim=1 방향으로 concatenate => return Tensor\n",
        "    hypothesises = result.max(-1)[1] # 확률이 제일 높은 index를 뽑아옴.\n",
        "\n",
        "    # loss 계산\n",
        "    targets = targets[:, 1:] # 0번째 column을 뺌 (모두 1임.)\n",
        "    loss = loss_func(result.contiguous().view(-1, result.size(-1)), targets.contiguous().view(-1))\n",
        "    step_loss = loss.item() # loss.item()은 loss의 스칼라 값.\n",
        "\n",
        "    # 정확도 계산\n",
        "    cer = charErrorRate(targets, hypothesises)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
        "\n",
        "    torch.cuda.empty_cache() # TODO : check that this is necessary\n",
        "\n",
        "    return step_loss, cer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEaSmddxL_h5"
      },
      "source": [
        "def validate(model, valid_dataset, device='cuda') :\n",
        "    print(\"[INFO] validate start\")\n",
        "    cer = 1.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad() :\n",
        "        inputs, input_lengths, targets, target_lengths = valid_dataset\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets[:, 1:].to(device)\n",
        "        model = model.to(device)\n",
        "\n",
        "        if isinstance(model, nn.DataParallel):\n",
        "            model.module.flatten_parameters()\n",
        "        else :\n",
        "            model.flatten_parameters()\n",
        "            \n",
        "        result = model(inputs, input_lengths)\n",
        "        result = torch.stack(result, dim=1).to(device)\n",
        "\n",
        "        hypothesises = result.max(-1)[1]\n",
        "        cer = charErrorRate(targets, hypothesises)\n",
        "    \n",
        "    return cer"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5jdW4rFwQB"
      },
      "source": [
        "def train(model, optimizer, train_path_list, valid_path_list, batch_size, num_epochs, n_mels, start_epoch, train_index, valid_index) :\n",
        "    print(\"[INFO] train start\")\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    valid_batch = valid_index\n",
        "    for epoch in range(start_epoch, num_epochs) :\n",
        "        # -> batch size만큼 train_path_list에서 가져오기\n",
        "        # 1. batch size만큼 data load하기 \n",
        "        # -> csv 파일 내에 있는 data path를 통해 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        epoch_cer = 1.0\n",
        "        train_step_num = 0\n",
        "        batch = train_index\n",
        "\n",
        "        start = time.time()\n",
        "        while batch <= len(train_path_list) :\n",
        "            audio_paths = train_path_list[batch-batch_size : batch]\n",
        "\n",
        "            # train\n",
        "            batch_loss, batch_cer = train_step(model, epoch, load_data(audio_paths, n_mels), loss_func, optimizer)\n",
        "            print(\"Batch {}\".format(batch))\n",
        "\n",
        "            batch += batch_size\n",
        "            epoch_loss += batch_loss\n",
        "            epoch_cer = batch_cer\n",
        "            train_step_num += 1\n",
        "\n",
        "            if (batch-batch_size) % 100 == 0 :\n",
        "                loss_cer = {\n",
        "                    'epoch' : epoch+1,\n",
        "                    'loss' : batch_loss,\n",
        "                    'tmp_cer' : batch_cer,\n",
        "                    'valid_cer' : 0.0\n",
        "                }\n",
        "\n",
        "                model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
        "                print(\"[INFO] Lastest checkpoint restored at batch {}!\".format(batch-batch_size))\n",
        "\n",
        "        # valid\n",
        "        if valid_batch <= len(valid_path_list) :\n",
        "            valid_paths = valid_path_list[valid_batch-batch_size : valid_batch]\n",
        "            valid_cer = validate(model, load_data(valid_paths, n_mels))\n",
        "            valid_batch += batch_size\n",
        "\n",
        "        print(\"Epoch {} Loss {:.4f} \\t train_cer {:.4f}% valid_cer {:.4f}%\".format(epoch+1, epoch_loss/train_step_num, epoch_cer, valid_cer))\n",
        "        print(\"Time taken for 1 epoch : {} secs\\n\".format(time.time() - start))\n",
        "\n",
        "        loss_cer = {\n",
        "            'epoch' : epoch+1,\n",
        "            'loss' : epoch_loss,\n",
        "            'tmp_cer' : epoch_cer,\n",
        "            'valid_cer' : valid_cer\n",
        "        }\n",
        "\n",
        "        # checkpoint 저장\n",
        "        if (epoch+1) % 5 == 0 :\n",
        "            model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
        "            print(\"[INFO] Lastest checkpoint restored!\")\n",
        "\n",
        "    model_save(model, optimizer, loss_cer, batch, valid_batch)\n",
        "    print(\"[INFO] Last checkpoint restored!\")\n",
        "    print(\"[INFO] Train Complete!! 🎶\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uceWMB3N7i9t"
      },
      "source": [
        "# tmp_input = torch.rand((BATCH_SIZE,951,N_MELS), dtype=torch.float64).uniform_(0,200)\n",
        "# tmp_input_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "# tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
        "# tmp_target_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "\n",
        "# tmp_input = tmp_input.float()\n",
        "# tmp_target = tmp_target.long()\n",
        "\n",
        "# train_dataset = (tmp_input, tmp_input_length, tmp_target, tmp_target_length)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIsWyc8HvXO1"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMNSrRyD73h"
      },
      "source": [
        "# hyper-parameter\n",
        "N_MELS = 80\n",
        "HIDDEN_DIM = 8\n",
        "DROPOUT_P = 0.15\n",
        "MAX_LEN = 150\n",
        "NUM_HEADS = 4\n",
        "ENC_NUM_LAYERS = 3\n",
        "DEC_NUM_LAYERS = 2\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = len(id2char) # dataset으로 label의 개수 넣어주기\n",
        "LEARNING_RATE = 1e-06\n",
        "WEIGHT_DECAY = 1e-05\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYkiah1V2rWT"
      },
      "source": [
        "#### 처음 학습시킬 때"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79"
      },
      "source": [
        "# # load data\n",
        "# train_path_list, valid_path_list = load_path_list(\"train_list_02.csv\", \"test_list_02.csv\")\n",
        "\n",
        "# # model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "# #                                   num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "# # optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "\n",
        "# model = LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "#                                   num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "# print(\"[INFO] model 초기화 성공\")\n",
        "\n",
        "# train(model, optimizer, train_path_list, valid_path_list, BATCH_SIZE, NUM_EPOCHS, N_MELS, 0, BATCH_SIZE, BATCH_SIZE)\n",
        "\n",
        "# assert False"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEA15Vw52uOL"
      },
      "source": [
        "#### checkpoint 가지고 학습시킬 때"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqBjNg5B2wrO",
        "outputId": "1bcbfa49-e239-42fc-f0da-103a084543da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load data\n",
        "train_path_list, valid_path_list = load_path_list(\"train_list_02.csv\", \"test_list_02.csv\")\n",
        "\n",
        "# model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "#                                   num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "# optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "model = LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "                                  num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# model load\n",
        "model_name = \"/content/drive/My Drive/SODA/backup/model_2020_11_10_06_24_59.pt\"\n",
        "checkpoint = model_load(model_name)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "train_index, valid_index, start_epoch = checkpoint['train_index'], checkpoint['valid_index'], checkpoint['epoch']\n",
        "\n",
        "print(\"[INFO] model load 성공\")\n",
        "\n",
        "train(model, optimizer, train_path_list, valid_path_list, BATCH_SIZE, NUM_EPOCHS, N_MELS, start_epoch-1, train_index, valid_index)\n",
        "\n",
        "assert False"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] load train path list from train_list_02.csv\n",
            "[INFO] load train path list from test_list_02.csv\n",
            "[INFO] model load 성공\n",
            "[INFO] train start\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Batch 3402\n",
            "Batch 3404\n",
            "Batch 3406\n",
            "Batch 3408\n",
            "Batch 3410\n",
            "Batch 3412\n",
            "Batch 3414\n",
            "Batch 3416\n",
            "Batch 3418\n",
            "Batch 3420\n",
            "Batch 3422\n",
            "Batch 3424\n",
            "Batch 3426\n",
            "Batch 3428\n",
            "Batch 3430\n",
            "Batch 3432\n",
            "Batch 3434\n",
            "Batch 3436\n",
            "Batch 3438\n",
            "Batch 3440\n",
            "Batch 3442\n",
            "Batch 3444\n",
            "Batch 3446\n",
            "Batch 3448\n",
            "Batch 3450\n",
            "Batch 3452\n",
            "Batch 3454\n",
            "Batch 3456\n",
            "Batch 3458\n",
            "Batch 3460\n",
            "Batch 3462\n",
            "Batch 3464\n",
            "Batch 3466\n",
            "Batch 3468\n",
            "Batch 3470\n",
            "Batch 3472\n",
            "Batch 3474\n",
            "Batch 3476\n",
            "Batch 3478\n",
            "Batch 3480\n",
            "Batch 3482\n",
            "Batch 3484\n",
            "Batch 3486\n",
            "Batch 3488\n",
            "Batch 3490\n",
            "Batch 3492\n",
            "Batch 3494\n",
            "Batch 3496\n",
            "Batch 3498\n",
            "Batch 3500\n",
            "model name : total_model_2020_11_10_06_36_47.pt\n",
            "[INFO] Lastest checkpoint restored at batch 3500!\n",
            "Batch 3502\n",
            "Batch 3504\n",
            "Batch 3506\n",
            "Batch 3508\n",
            "Batch 3510\n",
            "Batch 3512\n",
            "Batch 3514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f7701879486f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] model load 성공\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_path_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_MELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b68f53d8811d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_path_list, valid_path_list, batch_size, num_epochs, n_mels, start_epoch, train_index, valid_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_cer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-9e20d150ae4f>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(batch_data_path_list, n_mels)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data_path_list\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# audio to feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_to_featureVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mmfcc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minput_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-9e20d150ae4f>\u001b[0m in \u001b[0;36maudio_to_featureVector\u001b[0;34m(audio_path, n_mels, noise_injection)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_wn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# data to melspectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mwarped_masked_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_augment_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_warping_para\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# melspectrogram spec augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped_masked_spectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ortho'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# mel spectrogram to mfcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/specAugment/spec_augment_tensorflow.py\u001b[0m in \u001b[0;36mspec_augment\u001b[0;34m(mel_spectrogram, time_warping_para, frequency_masking_para, time_masking_para, frequency_mask_num, time_mask_num)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mwarped_mel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped_mel_spectrogram_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     warped_mel_spectrogram = warped_mel_spectrogram.reshape([warped_mel_spectrogram.shape[1],\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRYXjhUtNY4B"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuFO-Q-zqRz"
      },
      "source": [
        "# 음성 파일 하나 받아서 결과 보여주기\n",
        "# parameter : model path, audio_path\n",
        "# => print(결과)\n",
        "def test(model_path, audio_path, n_mels=80, device='cuda') :\n",
        "    # load audio => feature vector\n",
        "    feature_vector = audio_to_featureVector(audio_path, n_mels)\n",
        "    feature_vector = torch.FloatTensor(feature_vector).transpose(0, 1).to(device)\n",
        "\n",
        "    input = feature_vector.unsqueeze(0)\n",
        "    input_length = torch.IntTensor([len(feature_vector)]).to(device)\n",
        "\n",
        "    # load model\n",
        "    model = torch.load(model_path)\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.decoder.device = device\n",
        "        model.module.encoder.device = device\n",
        "    else:\n",
        "        model.encoder.device = device\n",
        "        model.decoder.device = device\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # validate처럼 넣어주고\n",
        "    result = model(inputs=input, input_lengths=input_length)\n",
        "    result = torch.stack(result, dim=1).to(device)\n",
        "    pred = result.max(-1)[1]\n",
        "\n",
        "    # 나온 output을 string으로 바꿔준 후\n",
        "    sentence = label_to_string(pred.cpu().detach().numpy())\n",
        "    # print함\n",
        "    print(sentence)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILxBdZEmedsl",
        "outputId": "4a96ec19-83a8-4367-b0e5-f18a011e9e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_path = '/content/drive/My Drive/SODA/backup/total_model_2020_11_10_06_36_47.pt'\n",
        "audio_path = '/content/drive/My Drive/SODA/data/original/KsponSpeech_02/KsponSpeech_0126/KsponSpeech_125001.pcm'\n",
        "\n",
        "test(model_path, audio_path)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['붐았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘았았켐맘맘맘']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZdF67_gkgN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}