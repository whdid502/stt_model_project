{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SODA_main_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nqHgOCkJec9C"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1ApGOIpngPOem8qf2KiA5sVLIVa8gtoYO",
      "authorship_tag": "ABX9TyMo0YK8K4AOBsUoJTxMSygs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/master/SODA_main_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGzNOhkolhMq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, optim\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7cVIQlwKPi"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbRK4o0qwPbf"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDFmoxfEwNjV"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYrXsg3wYLz"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool) :\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            # block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(num_features=512),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypewFuYwMEq"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        self.device = device\n",
        "\n",
        "        if self.extractor == 'vgg':\n",
        "            # input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            input_size = 1024\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "    conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "    batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "    conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "    if self.training:\n",
        "        self.rnn.flatten_parameters()\n",
        "\n",
        "    output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "    return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqHgOCkJec9C"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUweJ9vfCBRZ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask) :\n",
        "    scaled_attention_logits = torch.bmm(q, k.transpose(1,2)) / np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None :\n",
        "        scaled_attention_logits.masked_fill_(mask, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_attention_logits, -1)\n",
        "    output = torch.bmm(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vss_cRBJfcn"
      },
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "    def __init__(self, d_model=512, num_heads=8) :\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        self.wq = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wk = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.wv = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, d_model, bias=True) # ??\n",
        "\n",
        "    def forward(self, q, k, v, mask=None) :        \n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
        "\n",
        "        # split heads\n",
        "        q = q.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        k = k.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "        v = v.permute(2,0,1,3).contiguous().view(batch_size * self.num_heads, -1, self.depth)\n",
        "\n",
        "        if mask is not None :\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = scaled_attention.view(self.num_heads, batch_size, -1, self.depth)\n",
        "        scaled_attention = scaled_attention.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.linear(scaled_attention) # TODO : check\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSljKJ8nQMLE"
      },
      "source": [
        "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "# out, attn = temp_mha(y, y, y, mask=None)\n",
        "\n",
        "# display(out.shape, attn.shape)\n",
        "# display(y)\n",
        "# out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrOLCm4ehUP"
      },
      "source": [
        "## Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlqlQXEGQ_kW"
      },
      "source": [
        "class DecoderStep(nn.Module) :\n",
        "    def __init__(self, num_classes, LSTM_num=1, d_model=1024, num_heads=4, dropout_p=0.3, device='cuda'):\n",
        "        super(DecoderStep, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, d_model)\n",
        "        self.input_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.uniDirLSTM = nn.LSTM(input_size=d_model, hidden_size=d_model, num_layers=LSTM_num, bias=True, batch_first=True, dropout=dropout_p, bidirectional=False)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        \n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.linear2 = nn.Linear(d_model, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, input_var, hidden, enc_output) :\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        batch_size, output_lengths = input_var.size(0), input_var.size(1)\n",
        "\n",
        "        embedded = self.embedding(input_var).to(self.device)\n",
        "        embedded = self.input_dropout(embedded)\n",
        "\n",
        "        if self.training :\n",
        "            self.uniDirLSTM.flatten_parameters()\n",
        "\n",
        "        out1, hidden = self.uniDirLSTM(embedded, hidden)\n",
        "        \n",
        "        context, attn_weights_block = self.mha(out1, enc_output, enc_output) # (batch_size, target_seq_len, d_model)\n",
        "        out2 = self.layernorm1(context + out1).view(-1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        out_proj = self.linear1(out2)\n",
        "        output = self.layernorm2(out_proj + out2).view(batch_size, -1, self.d_model) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        output = self.linear2(torch.tanh(output).contiguous().view(-1, self.d_model))\n",
        "\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        output = output.view(batch_size, output_lengths, -1).squeeze(1)\n",
        "\n",
        "        return output, hidden, attn_weights_block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEVQ3IdEjkn"
      },
      "source": [
        "class Decoder(nn.Module) :\n",
        "    def __init__(self, num_classes, max_length=150, d_model=1024, num_heads=4, LSTM_num=2, dropout_p=0.3, device='cuda'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "        # self.num_layers = num_layers\n",
        "\n",
        "        self.dec_layer = DecoderStep(num_classes=num_classes, LSTM_num=LSTM_num, d_model=d_model, num_heads=num_heads, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, enc_outputs) :\n",
        "        assert enc_outputs is not None\n",
        "        \n",
        "        hidden = None\n",
        "        result = list()\n",
        "\n",
        "        batch_size = enc_outputs.size(0)\n",
        "\n",
        "        # validate\n",
        "        if inputs is None :\n",
        "            inputs = torch.LongTensor([1] * batch_size).view(batch_size, 1).to(self.device) # [sos_id] * batch_size\n",
        "            max_length = self.max_length\n",
        "        else :\n",
        "            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n",
        "\n",
        "        input_var = inputs[:, 0].unsqueeze(1)\n",
        "        for di in range(max_length) :\n",
        "            step_output, hidden, attn_weights_block = self.dec_layer(input_var, hidden, enc_outputs)\n",
        "            result.append(step_output)\n",
        "            input_var = result[-1].topk(1)[1]\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM11tHrMpzIR"
      },
      "source": [
        "## LAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19VWgRi5H-Vp"
      },
      "source": [
        "class LAS(nn.Module) :\n",
        "    def __init__(self, num_classes, input_size=80, hidden_dim=512, dropout_p=0.15, mask_conv=None, max_len=150, num_heads=4, \n",
        "                 dec_num_layers=2, enc_num_layers=3, device='cuda'):\n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder = Listener(input_size=input_size, hidden_dim=hidden_dim, device=device, dropout_p=dropout_p, num_layers=enc_num_layers)\n",
        "        self.decoder = Decoder(num_classes=num_classes, max_length=max_len, d_model=hidden_dim << 1, LSTM_num=dec_num_layers, dropout_p=dropout_p, device=device)\n",
        "\n",
        "    def forward(self, inputs, input_lengths, targets=None):\n",
        "        output, hidden = self.encoder(inputs, input_lengths)\n",
        "\n",
        "        result = self.decoder(targets, output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def flatten_parameters(self) :\n",
        "        self.encoder.rnn.flatten_parameters()\n",
        "        self.decoder.dec_layer.uniDirLSTM.flatten_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbf3XKq4D4bO"
      },
      "source": [
        "## data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYHyk6D1LN9"
      },
      "source": [
        "import csv\n",
        "\n",
        "# (train & valid) dataset path가 들어있는 csv 파일 읽어오기\n",
        "# parameter로 train path csv 경로 혹은 valid path csv 경로가 들어옴\n",
        "# => return data_path_list\n",
        "def load_path_list(train_path, valid_path) :\n",
        "    train_path_list, valid_path_list = [], []\n",
        "\n",
        "    print(f\"[INFO] load train path list from {train_path}\")\n",
        "    with open(train_path, 'r') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            train_path_list.append((line[0], line[1]))\n",
        "\n",
        "    print(f\"[INFO] load train path list from {valid_path}\")\n",
        "    with open(valid_path, 'r') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            valid_path_list.append((line[0], line[1]))\n",
        "\n",
        "    return train_path_list, valid_path_list\n",
        "\n",
        "# id, char 가 적혀있는 csv 파일 읽어오기\n",
        "# => return id2char (-> type : dictionary)\n",
        "def load_id2char(path) :\n",
        "    print(f\"[INFO] load id2char from {path}\")\n",
        "    \n",
        "    id2char = {}\n",
        "\n",
        "    with open(path, 'r', encoding='ms949') as f:\n",
        "        r = csv.reader(f)\n",
        "        next(r)\n",
        "        for line in r:\n",
        "            id2char[line[0]] = line[1]\n",
        "    \n",
        "    return id2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ygnl0Uk8_p1"
      },
      "source": [
        "#### feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmEcNMPA9Crt"
      },
      "source": [
        "!pip install tensorflow==1.13.2\n",
        "!pip3 install SpecAugment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DBbM9v3pHw3"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')\n",
        "from specAugment import spec_augment_tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDArguH69iEn"
      },
      "source": [
        "import librosa\n",
        "from scipy.fftpack import dct\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSd3pezNo63j"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')\n",
        "from specAugment import spec_augment_tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYULYIur8rZ_"
      },
      "source": [
        "# audio -> feature vector\n",
        "# => return feature vector\n",
        "def audio_to_featureVector(audio_path, n_mels, noise_injection=False) :\n",
        "    signal = np.memmap(os.path.join(os.getcwd(), 'original', audio_path), dtype='h', mode='r').astype('float32') # load audio\n",
        "    data = signal / 32767   # normalize audio\n",
        "\n",
        "    # noise injection\n",
        "    if noise_injection :\n",
        "        wn = np.random.randn(len(data))\n",
        "        data_wn = data + 0.005*wn\n",
        "    else:\n",
        "        data_wn = data\n",
        "\n",
        "    sr = 16000\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=data_wn, sr=sr, n_mels=256, hop_length=128, fmax=8000) # data to melspectrogram\n",
        "\n",
        "    warped_masked_spectrogram = spec_augment_tensorflow.spec_augment(mel_spectrogram=mel_spectrogram, time_warping_para=30) # melspectrogram spec augmentation\n",
        "\n",
        "    mfcc = dct(librosa.core.power_to_db(warped_masked_spectrogram), type=2, axis=1, norm='ortho')[:n_mels] # mel spectrogram to mfcc\n",
        "\n",
        "    return mfcc\n",
        "\n",
        "# 0. batch size만큼의 data path list 받아서 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "# 1. batch내의 max length로 padding\n",
        "# => return inputs, input_lengths, targets, target_lengths\n",
        "def load_data(batch_data_path_list, n_mels):\n",
        "    mfcc_list, input_lengths, target_list, target_lengths = [], [], [], []\n",
        "\n",
        "    max_mfcc_shape = 0\n",
        "    max_target_shape = 0\n",
        "    for i in batch_data_path_list :\n",
        "        # audio to feature vector\n",
        "        mfcc = audio_to_featureVector(i[0], n_mels, False) \n",
        "        mfcc_list.append(mfcc)\n",
        "        input_lengths.append(mfcc.shape[1])\n",
        "        # for padding\n",
        "        if max_mfcc_shape < mfcc.shape[1] :\n",
        "            max_mfcc_shape = mfcc.shape[1]\n",
        "\n",
        "        # target list\n",
        "        with open(os.path.join(os.getcwd(), 'original', i[1]), 'r') as f :\n",
        "            label = f.readline()\n",
        "            label = list(map(int, label.split()))\n",
        "            target_list.append(label)\n",
        "            target_lengths.append(len(label))\n",
        "            # for padding\n",
        "            if max_target_shape < len(label) :\n",
        "                max_target_shape = len(label)\n",
        "\n",
        "    # train padding\n",
        "    inputs = []\n",
        "    for mfcc in mfcc_list :\n",
        "        padding_shape = np.zeros((n_mels, max_mfcc_shape))\n",
        "        padding_shape[:mfcc.shape[0],:mfcc.shape[1]] = mfcc\n",
        "        inputs.append(padding_shape)\n",
        "\n",
        "    # target padding\n",
        "    padding_targets = []\n",
        "    for target in target_list :\n",
        "        target_padding_shape = np.zeros(max_target_shape+2)\n",
        "        target.insert(0,1) # 맨 앞에 sos_id 추가\n",
        "        target.append(2) # 맨 뒤에 eos_id 추가\n",
        "        target_padding_shape[:len(target)] = target\n",
        "        padding_targets.append(target_padding_shape)\n",
        "\n",
        "    inputs = torch.FloatTensor(inputs).permute(0,2,1)\n",
        "    input_lengths = torch.IntTensor(input_lengths)\n",
        "    padding_targets = torch.LongTensor(padding_targets)\n",
        "\n",
        "    return inputs, input_lengths, padding_targets, target_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8bi0wDfB4yM"
      },
      "source": [
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s \"/content/drive/My Drive/\" \"/myDrive\"\n",
        "!ls \"/myDrive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifPs1l3pUiR"
      },
      "source": [
        "%cd \"/myDrive/SODA/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkoAlbfPNL8"
      },
      "source": [
        "!pip install python-Levenshtein\n",
        "import Levenshtein as Lev\n",
        "\n",
        "total_dist = 0.0\n",
        "total_length = 0.0\n",
        "EOS_ID = 2\n",
        "id2char = load_id2char(\"aihub_labels_0001.csv\") # TODO -> parameter : id, char 가 적혀있는 csv 파일 읽어오기\n",
        "\n",
        "def label_to_string(labels) : \n",
        "    if len(labels.shape) == 1:\n",
        "        sentence = str()\n",
        "        for label in labels:\n",
        "            if label.item() == EOS_ID :\n",
        "                break\n",
        "            if id2char[str(label.item())] == '^' :\n",
        "                sentence += \"(웃음)\"\n",
        "            elif str(label.item()) == '3' :\n",
        "                sentence += \" \"\n",
        "            else :\n",
        "                sentence += id2char[str(label.item())]\n",
        "\n",
        "        return sentence\n",
        "\n",
        "    sentences = list()\n",
        "    for batch in labels:\n",
        "        sentence = str()\n",
        "        for label in batch:\n",
        "            if label.item() == EOS_ID :\n",
        "                break\n",
        "            if id2char[str(label.item())] == '^' :\n",
        "                sentence += \"(웃음)\"\n",
        "            elif str(label.item()) == '3' :\n",
        "                sentence += \" \"\n",
        "            else :\n",
        "                sentence += id2char[str(label.item())]\n",
        "        sentences.append(sentence)\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "# TODO\n",
        "# 1. 한글로 넣었을 때 distance 어떻게 나오는지 확인\n",
        "# 2. 아니면 형태소를 기준으로 정확도를 내는 라이브러리 찾아보기\n",
        "def charErrorRate(targets, hypothesises) :\n",
        "    total_dist = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for target, hypothesis in zip(targets, hypothesises) :\n",
        "        s1 = label_to_string(target)\n",
        "        s2 = label_to_string(hypothesis)\n",
        "\n",
        "        # space 제거\n",
        "        s1 = s1.replace(' ', '')\n",
        "        s2 = s2.replace(' ', '')\n",
        "\n",
        "        # '_' 제거\n",
        "        s1 = s1.replace('_', '')\n",
        "        s2 = s2.replace('_', '')\n",
        "\n",
        "        dist = Lev.distance(s2, s1)\n",
        "        length = len(s1)\n",
        "\n",
        "        total_dist += dist\n",
        "        total_length += length\n",
        "\n",
        "    return total_dist / total_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsApHJTBU0l"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ2pDfJGBWl6"
      },
      "source": [
        "CHECKPOINT_SAVE_PATH = '/content/drive/My Drive/SODA/backup'\n",
        "had_header = False\n",
        "\n",
        "def model_save(model, optimizer, loss_cer) :\n",
        "    global had_header\n",
        "    date_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())\n",
        "    \n",
        "    trainer_states = {\n",
        "        'optimizer' : optimizer,\n",
        "        'model' : model\n",
        "    }\n",
        "\n",
        "    torch.save(trainer_states, os.path.join(CHECKPOINT_SAVE_PATH, \"model_\"+date_time+\".pt\")) # model save\n",
        "\n",
        "    # loss & cer save\n",
        "    loss_cer['date_time'] = date_time\n",
        "    with open(os.path.join(CHECKPOINT_SAVE_PATH, 'loss_cer.csv'), 'a', encoding='ms949') as f :\n",
        "        w = csv.DictWriter(f, loss_cer.keys())\n",
        "\n",
        "        if not had_header :\n",
        "            w.writeheader()\n",
        "            had_header = True\n",
        "\n",
        "        w.writerow(loss_cer)\n",
        "\n",
        "def model_load(model_name) :\n",
        "    trainer_states = torch.load(os.path.join(CHECKPOINT_SAVE_PATH, model_name))\n",
        "\n",
        "    return trainer_states[model], trainer_states[optimizer]\n",
        "\n",
        "# TODO : checkpoint가 저장되는 형식보고 작성하기\n",
        "def model_load_latest(checkpoint_dir) :\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFtjdwSnG8Ik"
      },
      "source": [
        "## Train & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XlgaQuTJarq"
      },
      "source": [
        "def train_step(model, epoch, train_dataset, loss_func, optimizer, device='cuda') :\n",
        "    model.train() # model을 train mode로 변경\n",
        "    \n",
        "    inputs, input_lengths, targets, target_lengths = train_dataset\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.flatten_parameters()\n",
        "    else :\n",
        "        model.flatten_parameters()\n",
        "\n",
        "    result = model(inputs, input_lengths, targets)\n",
        "    result = torch.stack(result, dim=1).to(device) # list를 dim=1 방향으로 concatenate => return Tensor\n",
        "    hypothesises = result.max(-1)[1] # 확률이 제일 높은 index를 뽑아옴.\n",
        "\n",
        "    # loss 계산\n",
        "    targets = targets[:, 1:] # 0번째 column을 뺌 (모두 1임.)\n",
        "    loss = loss_func(result.contiguous().view(-1, result.size(-1)), targets.contiguous().view(-1))\n",
        "    step_loss = loss.item() # loss.item()은 loss의 스칼라 값.\n",
        "\n",
        "    # 정확도 계산\n",
        "    cer = charErrorRate(targets, hypothesises)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
        "\n",
        "    # torch.cuda.empty_cache() # TODO : check that this is necessary\n",
        "\n",
        "    return step_loss, cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEaSmddxL_h5"
      },
      "source": [
        "def validate(model, valid_dataset, device='cuda') :\n",
        "    print(\"[INFO] validate start\")\n",
        "    cer = 1.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad() :\n",
        "        inputs, input_lengths, targets, target_lengths = valid_dataset\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets[:, 1:].to(device)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # model.module.flatten_parameters()\n",
        "        model.flatten_parameters()\n",
        "        result = model(inputs, input_lengths)\n",
        "        result = torch.stack(result, dim=1).to(device)\n",
        "\n",
        "        hypothesises = result.max(-1)[1]\n",
        "        cer = charErrorRate(targets, hypothesises)\n",
        "    \n",
        "    return cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5jdW4rFwQB"
      },
      "source": [
        "def train(model, train_path_list, valid_path_list, batch_size, num_epochs, lr, weight_decay, n_mels) :    \n",
        "    print(\"[INFO] train start\")\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "    # optimizer = optim.Adam(model.module.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # valid_before = 0\n",
        "    valid_batch = batch_size\n",
        "\n",
        "    for epoch in range(num_epochs) :\n",
        "        # TODO\n",
        "        # -> batch size만큼 train_path_list에서 가져오기\n",
        "        # 1. batch size만큼 data load하기 \n",
        "        # -> csv 파일 내에 있는 data path를 통해 audio->feature vector & label & len(feature_vectors) & len(labels) 각각의 list를 을 하나의 tuple로 묶기\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        epoch_cer = 1.0\n",
        "        train_step_num = 0\n",
        "        batch = batch_size\n",
        "\n",
        "        start = time.time()\n",
        "        while batch <= len(train_path_list) :\n",
        "            audio_paths = train_path_list[batch-batch_size : batch]\n",
        "\n",
        "            # train\n",
        "            batch_loss, batch_cer = train_step(model, epoch, load_data(audio_paths, n_mels), loss_func, optimizer)\n",
        "\n",
        "            print(\"Batch {}\".format(batch))\n",
        "\n",
        "            batch += batch_size\n",
        "            epoch_loss += batch_loss\n",
        "            epoch_cer = batch_cer\n",
        "            train_step_num += 1\n",
        "\n",
        "        # valid\n",
        "        valid_paths = valid_path_list[valid_batch-batch_size : valid_batch]\n",
        "        valid_cer = validate(model, load_data(valid_paths, n_mels))\n",
        "\n",
        "        valid_batch += batch_size\n",
        "\n",
        "        print(\"Epoch {} Loss {:.4f} \\t train_cer {:.4f} valid_cer {:.4f}\".format(epoch+1, epoch_loss/train_step_num, epoch_cer, valid_cer))\n",
        "        print(\"Time taken for 1 epoch : {} secs\\n\".format(time.time() - start))\n",
        "\n",
        "        loss_cer = {\n",
        "            'epoch' : epoch+1,\n",
        "            'loss' : epoch_loss,\n",
        "            'epoch_cer' : epoch_cer,\n",
        "            'valid_cer' : valid_cer\n",
        "        }\n",
        "\n",
        "        # checkpoint 저장\n",
        "        if (epoch+1) % 5 == 0 :\n",
        "            model_save(model, optimizer, loss_cer)\n",
        "            print(\"[INFO] Lastest checkpoint restored!\")\n",
        "\n",
        "    model_save(model, optimizer, loss_cer)\n",
        "    print(\"[INFO] Last checkpoint restored!\")\n",
        "    print(\"[INFO] Train Complete!! 🎶\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uceWMB3N7i9t"
      },
      "source": [
        "# tmp_input = torch.rand((BATCH_SIZE,951,N_MELS), dtype=torch.float64).uniform_(0,200)\n",
        "# tmp_input_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "# tmp_target = torch.rand((BATCH_SIZE,59), dtype=torch.float64).uniform_(0,49)\n",
        "# tmp_target_length = torch.randint(0, N_MELS, size=(BATCH_SIZE,))\n",
        "\n",
        "# tmp_input = tmp_input.float()\n",
        "# tmp_target = tmp_target.long()\n",
        "\n",
        "# train_dataset = (tmp_input, tmp_input_length, tmp_target, tmp_target_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIsWyc8HvXO1"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMNSrRyD73h"
      },
      "source": [
        "# hyper-parameter\n",
        "N_MELS = 80\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT_P = 0.15\n",
        "MAX_LEN = 150\n",
        "NUM_HEADS = 4\n",
        "ENC_NUM_LAYERS = 3\n",
        "DEC_NUM_LAYERS = 2\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_CLASSES = len(id2char) # dataset으로 label의 개수 넣어주기\n",
        "LEARNING_RATE = 1e-06\n",
        "WEIGHT_DECAY = 1e-05\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnGe2zmHU79"
      },
      "source": [
        "# load data\n",
        "train_path_list, valid_path_list = load_path_list(\"train_list_0001.csv\", \"test_list_0001.csv\")\n",
        "\n",
        "# model = nn.DataParallel(LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "#                                   num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)).to('cuda')\n",
        "\n",
        "model = LAS(num_classes=NUM_CLASSES, input_size=N_MELS, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, max_len=MAX_LEN, \n",
        "                                  num_heads=NUM_HEADS, dec_num_layers=DEC_NUM_LAYERS, enc_num_layers=ENC_NUM_LAYERS, device=DEVICE)\n",
        "\n",
        "print(\"[INFO] model 초기화 성공\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1W0V17FGJM5"
      },
      "source": [
        "# !pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdpIx1H-FCSi"
      },
      "source": [
        "train(model, train_path_list, valid_path_list, BATCH_SIZE, 1, LEARNING_RATE, WEIGHT_DECAY, N_MELS)\n",
        "\n",
        "assert False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRYXjhUtNY4B"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuFO-Q-zqRz"
      },
      "source": [
        "# 음성 파일 하나 받아서 결과 보여주기\n",
        "# parameter : model path, audio_path\n",
        "# => print(결과)\n",
        "def test(model_path, audio_path, n_mels=80, device='cuda') :\n",
        "    # load audio => feature vector\n",
        "    feature_vector = audio_to_featureVector(audio_path, n_mels)\n",
        "    feature_vector = torch.FloatTensor(feature_vector).transpose(0, 1).to(device)\n",
        "\n",
        "    input = feature_vector.unsqueeze(0)\n",
        "    input_length = torch.IntTensor([len(feature_vector)]).to(device)\n",
        "\n",
        "    # load model\n",
        "    load_model = torch.load(model_path)\n",
        "    model = load_model['model']\n",
        "    # optimizer = load_model['optimizer']\n",
        "\n",
        "    # TODO : check that this is necessary\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model.module.decoder.device = device\n",
        "        model.module.encoder.device = device\n",
        "    else:\n",
        "        model.encoder.device = device\n",
        "        model.decoder.device = device\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # validate처럼 넣어주고\n",
        "    result = model(inputs=input, input_lengths=input_length)\n",
        "    result = torch.stack(result, dim=1).to(device)\n",
        "    pred = result.max(-1)[1]\n",
        "\n",
        "    # 나온 output을 string으로 바꿔준 후\n",
        "    sentence = label_to_string(pred.cpu().detach().numpy())\n",
        "    # print함\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILxBdZEmedsl"
      },
      "source": [
        "model_path = '/content/drive/My Drive/SODA/backup/model_2020_11_04_08_57_44.pt'\n",
        "audio_path = '/content/drive/My Drive/SODA/data/original/KsponSpeech_01/KsponSpeech_0001/KsponSpeech_000001.pcm'\n",
        "\n",
        "test(model_path, audio_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZdF67_gkgN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}