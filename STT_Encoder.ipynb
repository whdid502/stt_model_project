{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STT_Encoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8oEssMVkRLIGe20Ea8oF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whdid502/stt_model_project/blob/encoder/STT_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZuZUoRh08kC"
      },
      "source": [
        "# encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXfO-pNU1Zg0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from typing import Tuple, Optional, Any"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZnz7BZm1xL7"
      },
      "source": [
        "class Listener(BaseRNN):\n",
        "  def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state\n",
        "            device: str = 'cuda',                  # device - 'cuda' or 'cpu'\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            num_layers: int = 3,                   # number of RNN layers\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional encoder\n",
        "            rnn_type: str = 'lstm',                # type of RNN cell\n",
        "            extractor: str = 'vgg',                # type of CNN extractor\n",
        "            activation: str = 'hardtanh',          # type of activation function\n",
        "            mask_conv: bool = False                # flag indication whether apply mask convolution or not\n",
        "    ) -> None:\n",
        "        self.mask_conv = mask_conv\n",
        "        self.extractor = extractor.lower()\n",
        "        if self.extractor == 'vgg':\n",
        "            input_size = (input_size - 1) << 5 if input_size % 2 else input_size << 5\n",
        "            super(Listener, self).__init__(input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\n",
        "            self.conv = VGGExtractor(activation, mask_conv)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported Extractor : {0}\".format(extractor))\n",
        "\n",
        "  def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "            conv_feat = self.conv(inputs.unsqueeze(1), input_lengths).to(self.device)\n",
        "            conv_feat = conv_feat.transpose(1, 2)\n",
        "\n",
        "            batch_size, seq_length, num_channels, hidden_dim = conv_feat.size()\n",
        "            conv_feat = conv_feat.contiguous().view(batch_size, seq_length, num_channels * hidden_dim)\n",
        "\n",
        "            if self.training:\n",
        "                self.rnn.flatten_parameters()\n",
        "\n",
        "            output, hidden = self.rnn(conv_feat)\n",
        "\n",
        "            return output, hidden\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c81yVgR3dif"
      },
      "source": [
        "class BaseRNN(nn.Module):\n",
        "    supported_rnns = {\n",
        "        'lstm': nn.LSTM,\n",
        "        'gru': nn.GRU,\n",
        "        'rnn': nn.RNN\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,                       # size of input\n",
        "            hidden_dim: int = 512,                 # dimension of RNN`s hidden state vector\n",
        "            num_layers: int = 1,                   # number of recurrent layers\n",
        "            rnn_type: str = 'lstm',                # number of RNN layers\n",
        "            dropout_p: float = 0.3,                # dropout probability\n",
        "            bidirectional: bool = True,            # if True, becomes a bidirectional rnn\n",
        "            device: str = 'cuda'                   # device - 'cuda' or 'cpu'\n",
        "    ) -> None:\n",
        "        super(BaseRNN, self).__init__()\n",
        "        rnn_cell = self.supported_rnns[rnn_type]\n",
        "        self.rnn = rnn_cell(input_size, hidden_dim, num_layers, True, True, dropout_p, bidirectional)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlEOLMo-2jyh"
      },
      "source": [
        "class CNNExtractor(nn.Module):\n",
        "    supported_activations = {\n",
        "        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'elu': nn.ELU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
        "        'gelu': nn.GELU()\n",
        "    }\n",
        "\n",
        "    def __init__(self, activation: str = 'hardtanh') -> None:\n",
        "        super(CNNExtractor, self).__init__()\n",
        "        self.activation = CNNExtractor.supported_activations[activation]\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n76aFj720oF"
      },
      "source": [
        "class VGGExtractor(CNNExtractor):\n",
        "    def __init__(self, activation: str, mask_conv: bool):\n",
        "        super(VGGExtractor, self).__init__(activation)\n",
        "        self.mask_conv = mask_conv\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            self.activation,\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Optional[Any]:\n",
        "        conv_feat = self.conv(inputs)\n",
        "        output = conv_feat\n",
        "\n",
        "        return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5Epnml3xxH",
        "outputId": "a0fb9a75-961b-40d2-d126-239a6b5e7460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "x = torch.randn(20,40)\n",
        "listener = Listener(20)\n",
        "y = listener.forward(x, x.size())\n",
        "print(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5f7ce1bb01be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlistener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3bb56b8078fd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_dim, device, dropout_p, num_layers, bidirectional, rnn_type, extractor, activation, mask_conv)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vgg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListener\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGGExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7169e4aaf84b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_dim, num_layers, rnn_type, dropout_p, bidirectional, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m                   \u001b[0;31m# device - 'cuda' or 'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     ) -> None:\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mrnn_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_rnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnn_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ]
    }
  ]
}